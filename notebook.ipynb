{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtaining the Data/Data Augmentation\n",
    "\n",
    "- nfl.csv is adapted from a larger Kaggle Dataset (https://www.kaggle.com/datasets/maxhorowitz/nflplaybyplay2009to2016?select=NFL+Play+by+Play+2009-2018+%28v5%29.csv)\n",
    "- I created new columns to help identify potential patterns to predict whether a pass was a completion or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"nfl.csv\")\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "df = pd.get_dummies(df, columns=['pass_location', \"pass_length\"], drop_first=True)\n",
    "\n",
    "df[\"pass_difficulty\"] = df[\"air_yards\"] / (df[\"pass_length_short\"] + 1)\n",
    "df[\"time_pressure\"] = df[\"quarter_seconds_remaining\"] / (df[\"down\"] + 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "The data preprocessing steps include:\n",
    "\n",
    "1. Handling missing values.\n",
    "2. Encoding categorical variables\n",
    "3. Splitting the data into training and testing sets.\n",
    "4. Normalizing the input features.\n",
    "5. Handling class imbalance using SMOTE, since there tended to be more completions than incompletions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "\n",
    "df[\"passer_player_id\"] = encoder.fit_transform(df[\"passer_player_id\"])\n",
    "df[\"receiver_player_id\"] = encoder.fit_transform(df[\"receiver_player_id\"])\n",
    "df = df.fillna(0)\n",
    "\n",
    "\n",
    "corr_with_complete = df.corr()[\"complete_pass\"].sort_values(ascending=False)\n",
    "print(corr_with_complete)\n",
    "\n",
    "\n",
    "sns.heatmap(df.corr())\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X = df.drop(columns=[\"complete_pass\"], axis=1)\n",
    "y = df[\"complete_pass\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train = np.array(X_train).astype(np.float32)\n",
    "y_train = np.array(y_train).astype(np.float32)\n",
    "X_test = np.array(X_test).astype(np.float32)\n",
    "y_test = np.array(y_test).astype(np.float32)\n",
    "\n",
    "\n",
    "values, counts = np.unique(y_test, return_counts=True)\n",
    "print(dict(zip(values, counts)))\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train, y_train = smote.fit_resample(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "- Input layer with 22 features.\n",
    "- Three hidden layers with 256, 128, and 64 neurons respectively, each followed by batch normalization and ReLU activation.\n",
    "- Output layer with a sigmoid activation function.\n",
    "\n",
    "\n",
    "## Training\n",
    "\n",
    "The model is compiled with the Adam optimizer and binary cross-entropy loss. The learning rate is scheduled to reduce on plateau, and the model is trained for 10 epochs with a batch size of 32.\n",
    "\n",
    "## Evaluation\n",
    "\n",
    "The model is evaluated on the test set, and the test loss and accuracy are printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Input(shape=(22,)),\n",
    "    keras.layers.Dense(256, activation = \"relu\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(128, activation = \"relu\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(64, activation = \"relu\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(1, activation = \"sigmoid\")\n",
    "])\n",
    "\n",
    "\n",
    "lr_scheduler = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.0005)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size = 32, callbacks=[lr_scheduler])\n",
    "\n",
    "results = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {results[0]}\")\n",
    "print(f\"Test Accuracy: {results[1]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
